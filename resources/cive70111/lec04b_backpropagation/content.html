<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Backpropagation</title>
    <script src="../phoebe-js/mathhelper.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="../styles.css">
    <link rel="stylesheet" href="../phoebe-js/phoebe.css">
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="iframe-content">
        <div class="container">

        <div class="infotab">
            <div class="infotab-header">
                <!-- Tab buttons will be auto-generated -->
            </div>
            <div class="infotab-content">
                <div class="infotab-panel" data-tab-title="Introduction">
                    <strong>Backpropagation Algorithm</strong><br>
                    Backpropagation is the key algorithm that enables neural networks to learn from data. It efficiently computes gradients by propagating errors backward through the network, allowing us to update weights to minimize prediction errors.<br><br>

                    This demo shows how gradients flow backward through layers and how weights are updated to reduce prediction error.
                </div>

                <div class="infotab-panel" data-tab-title="Instructions">
                    <strong>How to Use This Demo:</strong><br>
                    • Set <em>input values</em> and <em>target output</em> to define the learning task<br>
                    • Adjust <em>learning rate</em> to control the step size for weight updates<br>
                    • Click <em>"Step Optimization"</em> to perform one iteration of backpropagation<br>
                    • Use <em>"Reset Weights"</em> to start with new random weights<br>
                    • Hover over table entries to highlight corresponding network connections<br>
                    • Watch the loss decrease over multiple optimization steps<br><br>

                    <strong>Network Configuration:</strong><br>
                    • Architecture: 2 inputs → 3 hidden (ReLU) → 1 output (linear)<br>
                    • Loss function: Mean Squared Error (MSE)<br>
                    • Optimization: Gradient descent with adjustable learning rate
                </div>

                <div class="infotab-panel" data-tab-title="Visual Guide">
                    <strong>Network Visualization:</strong><br>
                    • <em>Node values:</em> Show current activations after forward pass<br>
                    • <em>Connection thickness:</em> Represents weight magnitudes<br><br>

                    <strong>Parameter Tables:</strong><br>
                    • <em>Weights table:</em> Current weight values for all connections<br>
                    • <em>Hover highlighting:</em> Shows which connection each table entry represents<br><br>

                    <strong>Loss History Graph:</strong><br>
                    • Tracks MSE loss over optimization steps<br>
                    • Shows learning progress as loss decreases<br>
                    • Illustrates effect of learning rate on convergence
                </div>

                <div class="infotab-panel" data-tab-title="Tips">
                    <strong>Learning Rate Guidelines:</strong><br>
                    • Start with learning rates around 0.01-0.1<br>
                    • Too high: Loss may oscillate or diverge<br>
                    • Too low: Very slow convergence<br>
                    • Observe loss curve to assess if rate is appropriate<br><br>

                    <strong>Training Observations:</strong><br>
                    • Watch how gradients propagate backward through layers<br>
                    • Notice that output layer gradients are typically larger<br>
                    • Hidden layer gradients depend on both forward activations and backward error signals<br>
                    • ReLU activation sets gradients to zero for negative inputs (dead neurons)<br><br>

                    <strong>Practical Insights:</strong><br>
                    • Multiple steps usually needed to reach good solutions<br>
                    • Different input-target pairs will produce different gradient patterns<br>
                    • Weight initialization affects convergence speed and final solution<br>
                    • Real applications use mini-batches and advanced optimizers (Adam, RMSprop)<br><br>
                </div>
            </div>
        </div>

        <div class="demo-area" style="flex-direction: column;">
            <div style="display: flex; gap: 20px; width: 100%;">
                <div class="plot-container" style="flex: 1 1 auto;">
                    <div class="network-container" id="network-container" style="position: relative;">
                        <!-- Network diagram will be rendered here -->
                        <canvas id="gradient-flow-canvas" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; pointer-events: none; z-index: 100;"></canvas>
                    </div>
                </div>

                <div class="controls" style="flex: 0 0 320px; padding: 15px; min-width: 0; width: 320px;">
                    <!-- Loss Display -->
                    <div class="loss-display">
                        <div class="loss-value" id="loss-value">0.0000</div>
                        <div class="step-info">Step: <span id="step-count">0</span></div>
                    </div>

                    <!-- Buttons -->
                    <div class="control-group" style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 8px;">
                        <button class="step-btn" id="step-btn">Step Optim.</button>
                        <button class="auto-btn" id="auto-btn">Auto Optim.</button>
                        <button class="reset-btn" id="reset-btn">Reset</button>
                    </div>

                    <!-- Training Values -->
                    <div class="control-group">
                        <h3>Training Values</h3>
                        <div id="paramslider-input1"></div>
                        <div id="paramslider-input2"></div>
                        <div id="paramslider-target"></div>
                    </div>

                    <!-- Learning Rate -->
                    <div class="control-group">
                        <div id="paramslider-learning-rate"></div>
                    </div>
                </div>
            </div>

            <!-- Loss History and Network Parameters -->
            <div style="display: flex; gap: 20px; width: 100%; align-items: stretch; min-height: 350px;">
                <!-- Loss Graph -->
                <div class="control-group" style="flex: 1; display: flex; flex-direction: column;">
                    <h3>Loss History</h3>
                    <div class="loss-graph" style="flex: 1;">
                        <canvas id="loss-canvas"></canvas>
                    </div>
                </div>

                <!-- Weights Table -->
                <div class="right-panel-tables" style="flex: 1;">
                    <h3>Network Parameters</h3>
                    <div class="weights-table">
                        <div class="table-content" id="weights-table">
                            <!-- Weights table will be generated here -->
                        </div>
                    </div>
                </div>
            </div>

            <!-- Weight Updates Panel -->
            <div style="display: flex; gap: 20px; width: 100%; margin-top: 20px;">
                <div class="control-group" style="flex: 1; background: #f8f9fa; border: 2px solid #dee2e6; padding: 15px;">
                    <h3>Weight Updates (Last Step)</h3>
                    <div id="weight-updates-display" style="font-family: monospace; font-size: 12px; max-height: 200px; overflow-y: auto;">
                        <div style="color: #666; font-style: italic;">Click "Step Optimization" to see weight updates</div>
                    </div>
                </div>
            </div>

            <!-- Training History Table -->
            <div class="horizontal-scroll-container" style="width: 100%; max-width: 100%; overflow: hidden; box-sizing: border-box;">
                <div class="horizontal-scroll-title">Training History</div>
                <div class="horizontal-scroll" id="historyScroll">
                    <table class="horizontal-scroll-table" id="historyTable">
                        <tbody id="historyBody">
                            <tr class="history-row-step">
                                <th>Step</th>
                                <td class="current-iteration">0</td>
                            </tr>
                            <tr class="history-row-loss">
                                <th>Loss</th>
                                <td class="current-iteration">0.000</td>
                            </tr>
                            <tr class="history-row-prediction">
                                <th>Prediction</th>
                                <td class="current-iteration">0.000</td>
                            </tr>
                            <tr class="history-row-target">
                                <th>Target</th>
                                <td class="current-iteration">0.000</td>
                            </tr>
                            <tr class="history-row-delta-loss">
                                <th>$\Delta$Loss</th>
                                <td class="current-iteration">—</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </div>

    </div>
    </div>

    <script src="../phoebe-js/infotab.js"></script>
    <script src="../phoebe-js/paramslider.js"></script>
    <script src="script.js"></script>
</body>
</html>
