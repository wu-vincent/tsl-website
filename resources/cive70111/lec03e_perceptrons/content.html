<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Perceptrons</title>
    <script src="../phoebe-js/mathhelper.js"></script>
    <script src="../phoebe-js/canvashelper.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.js"></script>
    <link rel="stylesheet" href="../styles.css">
    <link rel="stylesheet" href="../phoebe-js/phoebe.css">
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="iframe-content">
        <div class="container">

        <div class="infotab">
            <div class="infotab-header">
            </div>

            <div class="infotab-content">
                <div class="infotab-panel active" data-tab-title="Introduction">
                    The perceptron is the simplest neural network, consisting of a single artificial neuron. Invented by Frank Rosenblatt in 1958, it was the first algorithm capable of learning to classify patterns from data.<br><br>
                    Perceptrons find applications in civil engineering for binary decision tasks like pass/fail structural assessment, safe/unsafe load classification, and go/no-go construction decisions.
                </div>

                <div class="infotab-panel" data-tab-title="Instructions">
                    • Click the plot to add blue (class 0) or red (class 1) points, or use the dataset buttons for predefined examples<br>
                    • Click <em>"Step"</em> to train one epoch—the perceptron tests each point and updates the decision boundary when it finds errors<br>
                    • Adjust learning rate $\eta$ to control update step size. Watch how the boundary moves to separate the classes<br><br>

                    <strong>How many epochs does it take to converge for linearly separable data? What happens with non-separable data?</strong>
                </div>

                <div class="infotab-panel" data-tab-title="Tips">
                    • <strong>Decision boundary:</strong> The line $\beta_0 + \beta_1 x_1 + \beta_2 x_2 = 0$ separating the two classes<br>
                    • <strong>Convergence:</strong> Training stops when no errors are made in an epoch (for linearly separable data)<br>
                    • <strong>Misclassified points:</strong> Highlighted during training to show which points caused weight updates<br>
                    • <strong>Learning rate:</strong> Higher $\eta$ means faster but potentially unstable learning<br>
                    • <strong>Perceptron guarantee:</strong> Will always converge to a solution if data is linearly separable
                </div>
            </div>
        </div>

        <div class="demo-area">
            <!-- Main visualization and controls container -->
            <div class="gd-container">
                <!-- Data View (2/3 width) -->
                <div class="view-panel">
                    <div class="view-title">Classification Plot</div>
                    <div class="plot-stage plot-stage--w-full plot-stage--h-450">
                        <canvas id="plotCanvas"></canvas>
                    </div>
                </div>

                <!-- Controls sidebar (1/3 width) -->
                <div class="controls">
                    <!-- Click Mode Selection -->
                    <div class="control-group">
                        <label class="control-label-strong">Click adds:</label>
                        <div class="class-selector">
                            <button class="class-btn active" id="class0Btn">Class 0</button>
                            <button class="class-btn" id="class1Btn">Class 1</button>
                        </div>
                    </div>

                    <!-- Clear Data Button -->
                    <div class="control-group">
                        <button class="action-btn" id="clearBtn">Clear All Data</button>
                    </div>

                    <!-- Dataset Selection -->
                    <div class="control-group">
                        <label class="control-label-strong">Or Select Dataset:</label>
                        <div class="dataset-grid">
                            <div class="dataset-box dataset-clean active" data-dataset="clean">
                                <div class="dataset-icon">✨</div>
                                <div class="dataset-label">Clean</div>
                            </div>
                            <div class="dataset-box dataset-xor" data-dataset="xor">
                                <div class="dataset-icon">❌</div>
                                <div class="dataset-label">XOR</div>
                            </div>
                        </div>
                    </div>

                    <!-- Perceptron Algorithm -->
                    <div class="control-group">
                        <label class="control-label-strong">Perceptron Algorithm:</label>
                        <button class="step-btn" id="stepBtn">Step</button>
                    </div>

                    <div class="control-group">
                        <button class="reset-btn" id="resetBtn">Reset</button>
                    </div>

                    <!-- Learning Rate -->
                    <div id="paramslider-eta"></div>
                </div>
            </div>

            <!-- Optimization History Table (full width) -->
            <div class="history-container">
                <div class="history-title">Training History</div>
                <div class="history-scroll" id="historyScroll">
                    <table class="history-table" id="historyTable">
                        <tbody id="historyBody">
                            <tr class="history-row-epoch">
                                <th>Epoch</th>
                            </tr>
                            <tr class="history-row-errors">
                                <th>Errors</th>
                            </tr>
                            <tr class="history-row-beta0">
                                <th>$\beta_0$ (bias)</th>
                            </tr>
                            <tr class="history-row-beta1">
                                <th>$\beta_1$</th>
                            </tr>
                            <tr class="history-row-beta2">
                                <th>$\beta_2$</th>
                            </tr>
                            <tr class="history-row-updates">
                                <th>Updates</th>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </div>

        <!-- Theory Section -->
        <div class="theory-section is-hidden">
            <h3>Mathematical Foundations</h3>
            <p>
                <strong>The Perceptron Algorithm</strong> is a supervised learning algorithm for binary linear classifiers. Given input $\mathbf{x} = (x_1, x_2)$ and weights $\boldsymbol{\beta} = (\beta_1, \beta_2)$ with bias $\beta_0$, the perceptron computes a <strong>weighted sum</strong>:<br>
                $$\mu = \beta_0 + \beta_1 x_1 + \beta_2 x_2 = \beta_0 + \boldsymbol{\beta}^T\mathbf{x}$$<br><br>

                The <strong>activation function</strong> applies a step function to produce binary output:<br>
                $$h(\mu) = \begin{cases} 1 & \text{if } \mu \geq 0 \\ 0 & \text{if } \mu < 0 \end{cases}$$<br><br>

                The perceptron geometrically defines a <strong>decision boundary</strong> (hyperplane) that separates the input space. Points on one side are classified as class 1, points on the other as class 0. The equation $\beta_0 + \beta_1 x_1 + \beta_2 x_2 = 0$ defines this boundary line.<br><br>

                <strong>The Learning Rule:</strong> For each training example $(\mathbf{x}^{(i)}, y^{(i)})$, the perceptron computes the prediction $\hat{y}^{(i)} = h(\mu^{(i)})$ and the error $e^{(i)} = y^{(i)} - \hat{y}^{(i)}$. If the prediction is correct ($e = 0$), no update is made. If incorrect ($e = \pm 1$), the weights are updated:<br>
                $$\beta_0 \leftarrow \beta_0 + \eta \cdot e^{(i)}$$
                $$\boldsymbol{\beta} \leftarrow \boldsymbol{\beta} + \eta \cdot e^{(i)} \cdot \mathbf{x}^{(i)}$$<br><br>

                where $\eta > 0$ is the <strong>learning rate</strong> controlling step size. This update rule geometrically rotates the decision boundary toward correctly classifying the misclassified point.<br><br>

                <strong>Training proceeds in epochs:</strong> each epoch processes all training examples once. The algorithm converges when an entire epoch produces zero errors. The <strong>Perceptron Convergence Theorem</strong> guarantees convergence in finite steps if the data is linearly separable. For non-separable data, the perceptron will never converge and continues updating indefinitely.
            </p>
        </div>

        <p class="demo-credits">
            Developed by <a href="https://transport-systems.imperial.ac.uk/members/qu-k" target="_blank">Kevin Yu</a> & <a href="https://transport-systems.imperial.ac.uk/members/angeloudis-p" target="_blank">Panagiotis Angeloudis</a>
        </p>

    </div>
    </div>

    <script src="../phoebe-js/infotab.js"></script>
    <script src="../phoebe-js/paramslider.js"></script>
    <script src="script.js"></script>
</body>
</html>
