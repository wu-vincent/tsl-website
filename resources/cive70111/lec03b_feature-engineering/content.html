<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Feature Engineering</title>
    <script src="../phoebe-js/mathhelper.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.js"></script>
    <link rel="stylesheet" href="../styles.css">
    <link rel="stylesheet" href="../phoebe-js/phoebe.css">
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="iframe-content">
        <div class="container">

        <div class="infotab">
            <div class="infotab-header">
            </div>

            <div class="infotab-content">
                <div class="infotab-panel active" data-tab-title="Introduction">
                    Multi-linear regression extends simple linear regression to handle multiple input features simultaneously. Instead of predicting from just one variable, we can use several features to make more accurate predictions.<br><br>
                    In civil engineering, this is particularly valuable for projects like predicting housing costs from multiple property characteristics, estimating structural loads from various design parameters, or forecasting traffic flow from multiple environmental factors.
                </div>

                <div class="infotab-panel" data-tab-title="Instructions">
                    â€¢ Click transformation boxes below each feature to activate/deactivate them (blue = active). Each feature can have multiple transformations active simultaneously.<br>
                    â€¢ By default, linear ($x$) is selected for all features. Try adding polynomial ($x^2$, $x^3$) or inverse ($1/x$, $1/x^2$) transformations to improve predictions.<br>
                    â€¢ The model automatically retrains when you toggle any transformation. Watch the metrics (MAE, MSE, RÂ²) to see how different transformations affect performance.<br>
                    â€¢ The scatter plot shows actual vs predicted pricesâ€”points closer to the diagonal line indicate better predictions.<br><br>

                    <strong>Can you find a feature engineering strategy that maximizes RÂ² beyond the linear baseline?</strong>

                </div>

                <div class="infotab-panel" data-tab-title="Tips">
                    <strong>Understanding the Results:</strong><br><br>
                    â€¢ <strong>RÂ² values:</strong> 0.0-0.3 (poor), 0.3-0.7 (moderate), 0.7+ (good fit)<br>
                    â€¢ <strong>MSE (Mean Squared Error):</strong> Lower values indicate better fit - measures average squared differences<br>
                    â€¢ <strong>MAE (Mean Absolute Error):</strong> More intuitive than MSE - average absolute prediction error<br>
                    â€¢ <strong>Feature importance:</strong> Notice which features contribute most to prediction accuracy<br>
                    â€¢ <strong>Overfitting vs Underfitting:</strong> Too few features may underfit; all features don't always improve performance
                </div>

                <div class="infotab-panel" data-tab-title="Lasso">
                    <strong>L1 Regularization (Lasso Regression)</strong><br><br>
                    Lasso adds an L1 penalty term to the loss function that encourages coefficient sparsity. As regularization strength increases, more coefficients are driven to exactly zero, automatically performing feature selection.<br><br>

                    â€¢ <strong>All features active:</strong> In this mode, all feature transformations are enabled<br>
                    â€¢ <strong>Adjust Î» (lambda):</strong> Use the strength buttons to control regularization intensity<br>
                    â€¢ <strong>Coefficient visualization:</strong> Feature panel shows coefficient values through color and saturation<br>
                    &nbsp;&nbsp;- <strong class="text-green">Green</strong>: Positive coefficients (saturation = magnitude)<br>
                    &nbsp;&nbsp;- <strong class="text-red">Red</strong>: Negative coefficients (saturation = magnitude)<br>
                    &nbsp;&nbsp;- <strong class="text-gray">Grey</strong>: Near-zero coefficients (driven to zero by regularization)<br><br>

                    <strong>Observe how higher Î» values create sparser models by eliminating less important features!</strong>
                </div>
            </div>
        </div>

        <div class="demo-area">
            <!-- Top section: Chart + Sidebar -->
            <div class="top-section">
                <!-- Chart -->
                <div class="plot-container">
                    <canvas id="prediction-plot"></canvas>
                </div>

                <!-- Sidebar with metrics -->
                <div class="metrics-sidebar">
                    <div class="metrics-panel">
                        <h3 class="section-heading">Metrics</h3>

                        <!-- Train/Test Toggle -->
                        <div class="dataset-toggle">
                            <div class="selectorbox dataset-box active" data-color="blue" id="toggle-train" data-icon="ðŸ“š" data-label="Train"></div>
                            <div class="selectorbox dataset-box" data-color="orange" id="toggle-test" data-icon="ðŸ”¬" data-label="Test"></div>
                        </div>

                        <div id="mae" class="metriclabel"></div>
                        <div id="mse" class="metriclabel"></div>
                        <div id="r2" class="metriclabel"></div>
                        <div id="feature-count" class="metriclabel"></div>
                    </div>
                </div>
            </div>

            <!-- Model controls row: buttons + loss curve -->
            <div class="model-controls-row">
                <!-- Model control buttons -->
                <div class="model-controls-panel">
                    <button class="model-btn" id="train-model-btn">Train Model</button>
                    <button class="model-btn reset-btn" id="reset-features-btn">Reset Features</button>

                    <!-- Lambda controls for Lasso mode (hidden by default) -->
                    <div class="lambda-controls is-hidden" id="lambda-controls">
                        <div class="lambda-label">Regularization Strength (Î»):</div>
                        <div class="lambda-buttons">
                            <button class="lambda-btn active" data-lambda="0">None</button>
                            <button class="lambda-btn" data-lambda="0.01">Low</button>
                            <button class="lambda-btn" data-lambda="0.1">Medium</button>
                            <button class="lambda-btn" data-lambda="1.0">High</button>
                        </div>
                    </div>
                </div>

                <!-- Loss curve -->
                <canvas id="loss-curve-canvas"></canvas>
            </div>

            <!-- Bottom section: Feature engineering panel -->
            <div class="feature-panel">
                <h3 class="section-heading">
                    Feature Engineering
                    <span id="training-badge" class="training-badge is-hidden">Training in Progress</span>
                    <span id="retrain-badge" class="retrain-badge is-hidden"></span>
                </h3>
                <div class="feature-engineering-grid">
                    <!-- Features will be dynamically generated -->
                </div>
            </div>
        </div>

        <!-- Theory Section -->
        <div class="theory-section is-hidden">
            <h3>Mathematical Foundations</h3>
            <p>
                <strong>Multiple linear regression</strong> extends the single-variable framework to handle multiple features simultaneously. With $p$ features, the linear model becomes $\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p$, where $\beta_0$ is the intercept (previously denoted $\alpha$ in single-variable regression) and $\beta_1, \ldots, \beta_p$ are the weights for each feature. <br><br>

                For our London housing dataset: $\widehat{\mathrm{SaleEstimate}} = \beta_0 + \beta_1 \mathrm{Latitude} + \beta_2 \mathrm{Longitude} + \beta_3 \mathrm{Bathrooms} + \beta_4 \mathrm{Bedrooms} + \beta_5 \mathrm{FloorArea} + \beta_6 \mathrm{LivingRooms} + \beta_7 \mathrm{DistanceToTube} + \beta_8 \mathrm{DistanceToBus} + \beta_9 \mathrm{BusStops}_{1\text{km}}$. Each feature contributes independently to the prediction, weighted by its corresponding parameter.<br><br>
            </p>
        </div>

        <p class="demo-credits">
            Developed by <a href="https://transport-systems.imperial.ac.uk/members/qu-k" target="_blank">Kevin Yu</a> & <a href="https://transport-systems.imperial.ac.uk/members/angeloudis-p" target="_blank">Panagiotis Angeloudis</a>
        </p>

        </div>
    </div>

    <script src="../phoebe-js/infotab.js"></script>
    <script src="../phoebe-js/metriclabel.js"></script>
    <script src="script.js"></script>
    <script>
        // Hook into MathJax's ready callback
        if (!window.MathJax) {
            window.MathJax = {};
        }
        if (!window.MathJax.startup) {
            window.MathJax.startup = {};
        }

        const originalPageReady = window.MathJax.startup.pageReady || (() => MathJax.startup.defaultPageReady());

        window.MathJax.startup.pageReady = function() {
            return originalPageReady.call(this).then(() => {
                console.log('MathJax is ready and has processed the page');
            });
        };

        // Initialize the demo when the page loads
        window.addEventListener('load', () => {
            new MultiLinearRegressionDemo();
        });
    </script>
</body>
</html>
