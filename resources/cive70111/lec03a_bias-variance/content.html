<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Higher-Order Features</title>
    <script src="../phoebe-js/mathhelper.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.js"></script>
    <link rel="stylesheet" href="../styles.css">
    <link rel="stylesheet" href="../phoebe-js/phoebe.css">
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="iframe-content">
        <div class="container">

        <!-- Info Tabs -->
        <div class="infotab">
            <div class="infotab-header">
            </div>

            <div class="infotab-content">
                <div class="infotab-panel active" data-tab-title="Introduction">
                    The bias-variance tradeoff is fundamental to understanding model complexity in machine learning. When fitting a polynomial to data, we face a critical choice: simple models (low degree) may underfit, while complex models (high degree) may overfit.<br><br>

                    This demo uses synthetic data (12 training points, 18 test points) with a complex underlying pattern combining polynomial and trigonometric components. You'll clearly see: degree 1 severely underfits (MSE ~8,500), degree 4 finds the sweet spot (MSE ~1,200), and degree 10 overfits dramatically (train MSE = 17, test MSE = 5,096!).
                </div>

                <div class="infotab-panel" data-tab-title="Instructions">
                    â€¢ Adjust polynomial degree slider (1-10) to see how complexity affects train vs test error<br>
                    â€¢ Check "Show Squared Errors" to visualize MSEâ€”area of squares = prediction errors<br>
                    â€¢ Click "Toggle Test Data" to show/hide test points<br>
                    â€¢ Watch the error chart: Test MSE drops sharply, then explodes at degree 8+<br><br>

                    <strong>Why does degree 4 minimize test error? Why does train error reach 0 but test error explodes?</strong>
                </div>

                <div class="infotab-panel" data-tab-title="Tips">
                    â€¢ <strong>Degree 1:</strong> Straight lineâ€”severe underfitting (MSE ~8,500)<br>
                    â€¢ <strong>Degree 2-3:</strong> Still underfitting (MSE ~2,900 â†’ ~260)<br>
                    â€¢ <strong>Degree 4:</strong> âœ“ SWEET SPOT (Test MSE ~1,200)â€”best generalization!<br>
                    â€¢ <strong>Degrees 5-8:</strong> Slight overfitting (test MSE ~1,200-1,400)<br>
                    â€¢ <strong>Degrees 9-10:</strong> Clear overfittingâ€”train MSE drops to 17, test MSE rises to ~5,000!<br>
                    â€¢ <strong>Balanced dataset:</strong> 12 training, 18 test points show realistic bias-variance progression
                </div>
            </div>
        </div>

        <!-- Demo Area -->
        <div class="demo-area">
            <!-- Chart: Scatter Plot with Polynomial Fit -->
            <div class="plot-container">
                <canvas id="scatterPlot"></canvas>
            </div>

            <!-- Controls -->
            <div class="controls">
                <div id="paramslider-degree"></div>

                <div class="metriclabel" id="equation">
                    $y = \beta_0 + \beta_1 x$
                </div>

                <div class="control-group">
                    <label><strong>Display Options:</strong></label>
                    <div class="selector-grid">
                        <div class="selectorbox" data-color="red" id="toggle-squared-errors" data-icon="ðŸ“" data-label="Squared Errors"></div>
                        <div class="selectorbox" data-color="orange" id="toggle-test-data" data-icon="ðŸ”¬" data-label="Test Data"></div>
                    </div>
                </div>

                <div class="metriclabel" id="train-mse">
                    MSE = 0.0
                </div>

                <div class="metriclabel" id="test-mse">
                    MSE = 0.0
                </div>
            </div>
        </div>

        <!-- Chart: Error Decomposition (full width below) -->
        <div class="plot-container plot-container--spaced">
            <div class="plot-stage plot-stage--w-full plot-stage--h-300">
                <canvas id="errorChart"></canvas>
            </div>
        </div>

        <!-- Theory Section -->
        <div class="theory-section is-hidden">
            <h3>Mathematical Foundations</h3>
            <p>
                <strong>Polynomial Regression</strong> models the relationship between temperature $x$ and bike rentals $y$ using a polynomial of degree $d$:
                $$\hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots + \beta_d x^d$$<br>

                Increasing $d$ allows more flexible curves but risks overfitting to noise.<br><br>

                <strong>Bias-Variance Decomposition</strong> breaks prediction error into three components:
                $$\text{Expected Error} = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}$$<br>

                <strong>BiasÂ²</strong> measures error from wrong model assumptions. High-bias models (low degree polynomials) are too simple to capture the true patternâ€”they systematically underpredict or overpredict. As model complexity increases, bias decreases.<br><br>

                <strong>Variance</strong> measures sensitivity to specific training data. High-variance models (high degree polynomials) fit training noise and vary wildly when trained on different samples. As model complexity increases, variance increases.<br><br>

                <strong>Irreducible Error</strong> is noise inherent in the data that no model can eliminate (weather variations, individual preferences, measurement errors).<br><br>

                <strong>The Tradeoff:</strong> Optimal model complexity balances bias and variance. Too simple = high bias (underfitting). Too complex = high variance (overfitting). The sweet spot minimizes total error on unseen test data.<br><br>

                <strong>Training vs Test Error:</strong> Training error always decreases with complexity (more parameters = better fit to training data). Test error follows a U-shaped curve: decreases initially (reducing bias), reaches minimum at optimal complexity, then increases (variance dominates). The gap between train and test error indicates overfittingâ€”larger gap means the model memorized training-specific patterns.
            </p>
        </div>

        <p class="demo-credits">
            Developed by <a href="https://transport-systems.imperial.ac.uk/members/qu-k" target="_blank">Kevin Yu</a> & <a href="https://transport-systems.imperial.ac.uk/members/angeloudis-p" target="_blank">Panagiotis Angeloudis</a>
        </p>

    </div>
    </div>

    <script src="../phoebe-js/infotab.js"></script>
    <script src="../phoebe-js/metriclabel.js"></script>
    <script src="../phoebe-js/paramslider.js"></script>
    <script src="../phoebe-js/linalg.js"></script>
    <script src="script.js"></script>
</body>
</html>
