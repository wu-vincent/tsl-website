<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gradient Descent</title>
    <script src="../phoebe-js/mathhelper.js"></script>
    <script src="../phoebe-js/canvashelper.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.js"></script>
    <link rel="stylesheet" href="../styles.css">
    <link rel="stylesheet" href="../phoebe-js/phoebe.css">
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="iframe-content">
        <div class="container">

        <div class="infotab">
            <div class="infotab-header">
            </div>

            <div class="infotab-content">
                <div class="infotab-panel active" data-tab-title="Introduction">
                    This interactive demo shows how gradient descent optimizes linear regression parameters. Unlike analytical solutions, gradient descent iteratively improves parameters by following the negative gradient of the cost function.<br><br>
                    You'll see the algorithm step-by-step, watching how the regression line evolves and tracking the optimization path on the cost surface. This is the foundation for training machine learning models.
                </div>

                <div class="infotab-panel" data-tab-title="Instructions">
                    • Click <em>"Step"</em> to perform one gradient descent iteration. Watch the regression line evolve (left panel) and the optimization path on the cost surface (right panel)<br>
                    • Use <em>"Re-initialize"</em> to start from new random parameters, or load different datasets to explore various optimization landscapes<br>
                    • Adjust the learning rate $\eta$ to see different convergence behaviors—too small is slow, too large causes oscillation<br><br>

                    <strong>What learning rate gives the fastest convergence without overshooting?</strong> Try values from 0.01 to 0.5.
                </div>

                <div class="infotab-panel" data-tab-title="Visual Guide">
                    • <em>Data & Regression Line (top-left):</em> Normalized data points, current regression line, residuals (dashed red lines showing prediction errors)<br>
                    • <em>Cost Surface (top-right):</em> Cost surface contours, current position (red dot), optimization path (yellow line)<br>
                    • <em>Cost Improvement (bottom):</em> Bar chart showing $|\Delta\mathcal{J}|$ (absolute cost reduction) for each iteration. Watch how the improvements diminish as the algorithm converges to the optimal solution<br>
                    • <em>Readouts:</em> Current iteration, normalized parameters $\alpha$ and $\beta$, cost $\mathcal{J}$, and gradient norm $||\nabla\mathcal{J}||$<br>
                    • <em>Caption:</em> Shows what happened in each step (gradient computation, parameter updates, and $\Delta\mathcal{J}$)<br>
                    • As convergence approaches, both the gradient norm and $|\Delta\mathcal{J}|$ decrease toward zero
                </div>
            </div>
        </div>

        <div class="demo-area layout-stacked">
            <!-- Visualization Panels at Top -->
            <div class="gd-container">
                <!-- Data View -->
                <div class="view-panel">
                    <div class="view-title">Data & Regression Line</div>
                    <canvas id="dataCanvas" style="width: 100%; height: 440px;"></canvas>
                    <div class="step-controls">
                        <button class="step-btn" id="stepBtn">Step</button>
                        <button class="reset-btn" id="resetBtn">Re-initialize</button>
                    </div>
                </div>

                <!-- Right column container for stacked charts -->
                <div class="right-column">
                    <!-- Cost Surface View -->
                    <div class="view-panel">
                        <div class="view-title">Cost Surface</div>
                        <canvas id="costCanvas" style="width: 100%; height: 300px;"></canvas>
                    </div>

                    <!-- Delta J Chart -->
                    <div class="view-panel">
                        <div class="view-title">Cost Improvement per Step (ΔJ)</div>
                        <canvas id="deltaJCanvas" style="width: 100%; height: 180px;"></canvas>
                    </div>
                </div>
            </div>

            <!-- Optimization History Table -->
            <div class="horizontal-scroll-container">
                <div class="horizontal-scroll-title">Optimization History</div>
                <div class="horizontal-scroll" id="historyScroll">
                    <table class="horizontal-scroll-table" id="historyTable">
                        <tbody id="historyBody">
                            <tr class="history-row-iteration">
                                <th>Iteration</th>
                                <td class="current-iteration">0</td>
                            </tr>
                            <tr class="history-row-alpha">
                                <th>$\alpha$</th>
                                <td class="current-iteration">0.000</td>
                            </tr>
                            <tr class="history-row-beta">
                                <th>$\beta$</th>
                                <td class="current-iteration">0.000</td>
                            </tr>
                            <tr class="history-row-cost">
                                <th>Cost $\mathcal{J}$</th>
                                <td class="current-iteration">0.000</td>
                            </tr>
                            <tr class="history-row-delta">
                                <th>$\Delta\mathcal{J}$</th>
                                <td class="current-iteration">—</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <!-- Controls Section at Bottom -->
            <div class="controls-bottom">
                <div class="controls-grid">
                    <!-- Column 1: Dataset Selection and Current State -->
                    <div>
                        <div class="control-group">
                            <label style="font-weight: bold; margin-bottom: 10px; display: block;">Dataset Selection</label>
                            <div class="dataset-controls">
                                <div class="dataset-buttons">
                                    <button class="dataset-btn structures-btn" id="structures-btn">Structures</button>
                                    <button class="dataset-btn geotech-btn" id="geotech-btn">Geotechnics</button>
                                    <button class="dataset-btn transport-btn" id="transport-btn">Transport</button>
                                    <button class="dataset-btn environment-btn" id="environment-btn">Environment</button>
                                </div>
                            </div>
                        </div>

                        <div class="control-group" style="margin-top: 20px;">
                            <label style="font-weight: bold; margin-bottom: 10px; display: block;">Current State</label>
                            <div class="readout-panel readout-grid">
                                <div class="readout-row">
                                    <span class="readout-label">Iteration:</span>
                                    <span class="readout-value" id="iter">0</span>
                                </div>
                                <div class="readout-row">
                                    <span class="readout-label">$\alpha$:</span>
                                    <span class="readout-value" id="alphaOut">0.000</span>
                                </div>
                                <div class="readout-row">
                                    <span class="readout-label">Cost $\mathcal{J}$:</span>
                                    <span class="readout-value" id="jOut">0.000</span>
                                </div>
                                <div class="readout-row">
                                    <span class="readout-label">$\beta$:</span>
                                    <span class="readout-value" id="betaOut">0.000</span>
                                </div>
                            </div>
                        </div>
                    </div>

                    <!-- Column 2: Initial Parameters -->
                    <div>
                        <div class="control-group">
                            <label style="font-weight: bold; margin-bottom: 10px; display: block;">Initial Parameters</label>
                            <div id="paramslider-alpha0"></div>
                            <div id="paramslider-beta0"></div>
                            <div id="paramslider-eta"></div>
                        </div>
                    </div>
                </div>
        </div>
        </div>

        <!-- Theory Section -->
        <div class="theory-section">
            <h3>Mathematical Foundations</h3>
            <p>
                <strong>Gradient descent</strong> is an iterative optimization algorithm that finds parameter values minimizing the cost function $\mathcal{J}(\alpha,\beta) = \frac{1}{2n}\sum_{i=1}^{n}(\alpha + \beta x_i - y_i)^2$ for our linear regression model $\mu(x) = \alpha + \beta x$. Rather than solving analytically, gradient descent starts with an initial guess for the parameters and repeatedly adjusts them in the direction that most decreases the cost.<br><br>

                At each iteration, we <strong>update</strong> the parameters according to $\alpha \leftarrow \alpha - \eta \frac{\partial \mathcal{J}}{\partial \alpha}$ and $\beta \leftarrow \beta - \eta \frac{\partial \mathcal{J}}{\partial \beta}$, where the gradients are $\frac{\partial \mathcal{J}}{\partial \alpha} = \frac{1}{n}\sum_{i=1}^{n}(\alpha + \beta x_i - y_i)$ and $\frac{\partial \mathcal{J}}{\partial \beta} = \frac{1}{n}\sum_{i=1}^{n}(\alpha + \beta x_i - y_i) \cdot x_i$.<br><br>

                The <strong>learning rate</strong> $\eta$ is crucial: too small and the algorithm takes tiny steps requiring many iterations; too large and it may overshoot the minimum and diverge. A moderate $\eta$ leads to steady decrease in $\mathcal{J}$ over iterations. As we approach the minimum, the gradient magnitude decreases, effectively reducing step size even with constant $\eta$.
            </p>
        </div>

        <p style="font-style: italic; text-align: right; margin-top: 10px; color: #666; font-size: 14px;">
            Developed by <a href="https://transport-systems.imperial.ac.uk/members/qu-k" target="_blank" style="color: #666; text-decoration: underline;">Kevin Yu</a> & <a href="https://transport-systems.imperial.ac.uk/members/angeloudis-p" target="_blank" style="color: #666; text-decoration: underline;">Panagiotis Angeloudis</a>
        </p>

    </div>
    </div>

    <script src="../phoebe-js/infotab.js"></script>
    <script src="../phoebe-js/paramslider.js"></script>
    <script src="script.js"></script>
</body>
</html>