<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neural Networks</title>
    <script src="../phoebe-js/mathhelper.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.js"></script>
    <link rel="stylesheet" href="../styles.css">
    <link rel="stylesheet" href="../phoebe-js/phoebe.css">
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <div class="iframe-content">
        <div class="container">

        <div class="infotab">
            <div class="infotab-header">
                <!-- Tab buttons will be auto-generated -->
            </div>
            <div class="infotab-content">
                <div class="infotab-panel" data-tab-title="Introduction">
                    <strong>Neural Network Architecture</strong><br>
                    Neural networks are computing systems inspired by biological neural networks. They consist of interconnected nodes (neurons) organized in layers that transform input data through weighted connections and activation functions.<br><br>

                    This demo lets you explore how different architectures and activation functions affect information flow through the network.
                </div>

                <div class="infotab-panel" data-tab-title="Instructions">
                    <strong>How to Use This Demo:</strong><br>
                    • Adjust the <em>input values</em> using the sliders or enter custom values<br>
                    • <em>Click on any node</em> to view and modify its incoming weights<br>
                    • Use <em>Add Layer</em> and <em>Remove Layer</em> buttons to change network architecture<br>
                    • Change the <em>number of units</em> in each hidden layer using the controls<br>
                    • Select different <em>activation functions</em> for each layer to see their effects<br>
                    • Watch how values propagate through the network in real-time<br>
                    • Use <em>"Generate Random Inputs"</em> to test with different input combinations<br><br>

                    <strong>Interactive Features:</strong><br>
                    • Weight editing: Click any neuron to modify its incoming weights<br>
                    • Architecture modification: Add/remove layers dynamically<br>
                    • Layer customization: Change neuron counts and activation functions
                </div>

                <div class="infotab-panel" data-tab-title="Visual Guide">
                    <strong>Layer Types:</strong><br>
                    • <strong style="color: #1976d2;">Input Layer:</strong> 3 input values that you control<br>
                    • <strong style="color: #7b1fa2;">Hidden Layers:</strong> Transform inputs using weights and activation functions<br>
                    • <strong style="color: #388e3c;">Output Layer:</strong> Single output value representing the network's prediction<br><br>

                    <strong>Visual Elements:</strong><br>
                    • <em>Node borders</em> are green for positive, red for negative values<br>
                    • <em>Connection color</em> represents weight magnitudes<br>
                </div>

                <div class="infotab-panel" data-tab-title="Tips">
                    <strong>Architecture Guidelines:</strong><br>
                    • Start with 1-2 hidden layers for most problems<br>
                    • Use 10-100 neurons per layer as a starting point<br>
                    • Deeper networks can model more complex relationships but may be harder to train<br>
                    • Width vs depth trade-offs: wider layers vs more layers<br><br>

                    <strong>Activation Function Selection:</strong><br>
                    • ReLU is most common for hidden layers (fast computation, avoids vanishing gradients)<br>
                    • Sigmoid for binary classification output layers<br>
                    • Linear for regression output layers<br>
                    • Tanh sometimes better than sigmoid in hidden layers (zero-centered)<br><br>

                    <strong>Practical Considerations:</strong><br>
                    • Observe how different architectures affect the output for the same inputs<br>
                    • Notice how activation functions shape the transformation at each layer<br>
                    • Experiment with weight values to understand their impact<br>
                    • Real networks require training data and optimization algorithms
                </div>
            </div>
        </div>

        <!-- Input Controls Panel -->
        <div class="input-panel">
            <div id="paramslider-input1"></div>
            <div id="paramslider-input2"></div>
            <div id="paramslider-input3"></div>
            <button class="generate-data-btn" id="random-inputs-btn">Generate Random Inputs</button>
        </div>

        <div class="demo-area">
            <div class="plot-container" style="flex: 1.95;">
                <div class="network-container" id="network-container">
                    <!-- Network diagram will be rendered here -->
                </div>
            </div>

            <div class="controls" style="flex: 1.15;">
                <!-- Network Architecture Controls -->
                <div class="control-group">
                    <h3>Network Architecture</h3>
                    <button class="btn-warning" id="randomise-weights-btn">Randomise Weights & Biases</button>
                    <div class="layer-controls" id="layer-controls">
                        <!-- Dynamic layer controls will be inserted here -->
                    </div>
                    <div class="layer-buttons">
                        <button class="add-layer-btn" id="add-layer-btn">+ Add Layer</button>
                        <button class="remove-layer-btn" id="remove-layer-btn">- Remove Layer</button>
                    </div>

                    <!-- Weight Legend -->
                    <div class="weight-legend">
                        <div class="legend-title">Weights & Biases</div>
                        <div class="legend-gradient"></div>
                        <div class="legend-labels">
                            <span>Negative</span>
                            <span>0</span>
                            <span>Positive</span>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Weight editing modal -->
        <div class="modal" id="weight-modal">
            <div class="modal-content">
                <h3 id="modal-title">Edit Weights</h3>
                <div id="modal-weights">
                    <!-- Weight inputs will be dynamically generated -->
                </div>
                <div style="text-align: right; margin-top: 20px;">
                    <button onclick="closeWeightModal()">Close</button>
                    <button onclick="randomizeWeights()" style="margin-left: 10px;">Randomize</button>
                </div>
            </div>
        </div>

    </div>
    </div>

    <script src="../phoebe-js/infotab.js"></script>
    <script src="../phoebe-js/paramslider.js"></script>
    <script src="script.js"></script>
</body>
</html>